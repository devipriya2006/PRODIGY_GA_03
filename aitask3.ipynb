{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSVnBdv/vftqKfOSgRwwcU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devipriya2006/PRODIGY_GA_03/blob/main/aitask3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text generation with Markovify**\n",
        "## Implementing a simple text generation algorithm using Markov chains. This task involves creating a statistical model that predicts the probability of a character or word based on the previous one(s)."
      ],
      "metadata": {
        "id": "JCpqIXMggAMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working with text files"
      ],
      "metadata": {
        "id": "r1ftSIj0k4A1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we get started, we'll first need some text! Grab two plain text files from Project Gutenberg (or from another source of your choice) and save them to the same directory as this notebook. (I suggest working with two files because we'll be running some code explicitly to \"compare\" two texts. Also, I think seeing two different outputs from the text generation methods discussed in this notebook will help you better understand how those methods work.) The code in the following cell loads into Python variables the contents of two plain text files, assigned to variables text_a and text_b. You'll need to replace the filenames with the names of the files that you downloaded, keeping the quotation marks (\") intact."
      ],
      "metadata": {
        "id": "eDbN4sa1nwt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_a = open(\"book1.txt\").read()\n",
        "text_b = open(\"book2.txt\").read()"
      ],
      "metadata": {
        "id": "r2j9DwgOiV81"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These variables are strings, which are essentially just long lists of the characters that occur in the text, in the order that they occur. The code in the following cell shows the first two hundred characters of text A:"
      ],
      "metadata": {
        "id": "h8cYD9uJn6FV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_a[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7y-FHFUrf5Xc",
        "outputId": "7979edef-f202-4048-d9c9-dfb646110938"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿The Project Gutenberg eBook of Aaron the Jew: A Novel\n",
            "    \n",
            "This ebook is for the use of anyone anywhere in the United States and\n",
            "most other parts of the world at no cost and with almost no restrictio\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can change text_a to text_b to see the output from your second text, or change 200 to a number of your choosing.\n",
        "\n",
        "The random.sample() function gives us a random sampling of the contents of a variable (as long as that variable is a sequence of things, like a string or a list). So, for example, to see twenty random characters from text B:"
      ],
      "metadata": {
        "id": "3UBluYkroKop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.sample(text_b, 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KnLRpyboAA4",
        "outputId": "77446d94-8c81-4464-d333-4cb036d99671"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['u',\n",
              " ' ',\n",
              " 'a',\n",
              " 'l',\n",
              " 'h',\n",
              " ' ',\n",
              " 'r',\n",
              " 'r',\n",
              " 'l',\n",
              " 'y',\n",
              " 'm',\n",
              " ',',\n",
              " 'l',\n",
              " 'y',\n",
              " 'b',\n",
              " 's',\n",
              " '.',\n",
              " ' ',\n",
              " 'e',\n",
              " 'o']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perhaps more interesting would be to see a randomly-sampled list of words. To do this, we'll make separate variables for the words in the text, using a Python function called .split(), which takes a string and turns it into a list of words contained in that string. The following cell makes two new variables that contain the words from both texts respectively:"
      ],
      "metadata": {
        "id": "fV_WyMMEohhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a_words = text_a.split()\n",
        "b_words = text_b.split()"
      ],
      "metadata": {
        "id": "tbwbSsTPoOON"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, ten random words from both text A and text B:"
      ],
      "metadata": {
        "id": "Lw5xFDGiosHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.sample(a_words, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIUu9jHgoksW",
        "outputId": "345f64b4-a944-4da2-bbaf-1e6097daa6af"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['through',\n",
              " 'with',\n",
              " 'never',\n",
              " 'match,',\n",
              " 'child',\n",
              " 'beloved!',\n",
              " 'in',\n",
              " \"affliction?'\",\n",
              " 'a',\n",
              " 'that']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.sample(b_words, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwRYvH0nooTu",
        "outputId": "862286fa-a049-4f28-c86d-2e6af6b62c1f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['and',\n",
              " 'in',\n",
              " 'any',\n",
              " 'learn',\n",
              " 'based',\n",
              " 'any',\n",
              " 'facility:',\n",
              " 'Gutenberg™.',\n",
              " 'a',\n",
              " 'the']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code in the following cell uses Python's Counter object to count the most common letters in the first of these texts:"
      ],
      "metadata": {
        "id": "P-fFZaSio5c4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "Counter(text_a).most_common(12)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCnVk7YFo15H",
        "outputId": "a9464086-dcf7-480c-b4d1-5db8768a27d3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(' ', 120468),\n",
              " ('e', 66340),\n",
              " ('t', 44644),\n",
              " ('o', 41780),\n",
              " ('a', 40269),\n",
              " ('n', 36241),\n",
              " ('h', 34379),\n",
              " ('i', 34318),\n",
              " ('s', 33532),\n",
              " ('r', 31302),\n",
              " ('d', 23476),\n",
              " ('l', 19907)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specifying the a_words variable gives the most frequent words instead:"
      ],
      "metadata": {
        "id": "ah0pgjSOpC3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(a_words).most_common(12)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPgB2aM9o9vu",
        "outputId": "6b200a1b-cabd-4075-a522-c26361dcfdbb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 5610),\n",
              " ('to', 3848),\n",
              " ('and', 3694),\n",
              " ('of', 3316),\n",
              " ('a', 2719),\n",
              " ('in', 2221),\n",
              " ('was', 1890),\n",
              " ('he', 1773),\n",
              " ('I', 1712),\n",
              " ('his', 1473),\n",
              " ('not', 1246),\n",
              " ('that', 1237)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare these to the most common words in text B:\n",
        "\n"
      ],
      "metadata": {
        "id": "i6lb2aS2pKIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(b_words).most_common(12)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAkq5P2LpHBH",
        "outputId": "cec7b540-fc77-4687-c8a3-f5dd411eb51f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 209),\n",
              " ('of', 134),\n",
              " ('and', 113),\n",
              " ('to', 109),\n",
              " ('in', 84),\n",
              " ('Project', 79),\n",
              " ('you', 73),\n",
              " ('a', 72),\n",
              " ('or', 71),\n",
              " ('with', 55),\n",
              " ('Gutenberg™', 53),\n",
              " ('this', 43)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generating with Markovify**\n",
        "To install Markovify on your computer, run the cell below:"
      ],
      "metadata": {
        "id": "25ubhiW_pb3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install markovify\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iB8UF2iZpNvf",
        "outputId": "21b64ad0-6f7c-4f38-b02b-b654608778c1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting markovify\n",
            "  Downloading markovify-0.9.4.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting unidecode (from markovify)\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: markovify\n",
            "  Building wheel for markovify (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for markovify: filename=markovify-0.9.4-py3-none-any.whl size=18607 sha256=b4e3764ae40198a43c5a1e47f1b68ab573e5bbde067e1e4e5b4d0a755d1bafdc\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/20/eb/1a3fb93f3132f2f9683e4efd834800f80c53aeddf50e84ae80\n",
            "Successfully built markovify\n",
            "Installing collected packages: unidecode, markovify\n",
            "Successfully installed markovify-0.9.4 unidecode-1.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import markovify"
      ],
      "metadata": {
        "id": "np6x6--oppfn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code in the following cell creates a new text generator, using the text in the variable specified to build the Markov model, which is then assigned to the variable generator_a."
      ],
      "metadata": {
        "id": "m3YNTLFMp39E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator_a = markovify.Text(text_a)"
      ],
      "metadata": {
        "id": "cFfpuFGapuuA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can then call the .make_sentence() method to generate a sentence from the model:"
      ],
      "metadata": {
        "id": "BW7DMtzNp-ja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(generator_a.make_sentence())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdvPnkuOpx19",
        "outputId": "b527b367-0684-4802-d080-2fb5f088ef94"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thus, we do not deny it; he undertook to find a home and an expression of her feelings it seemed as if for guidance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The .make_short_sentence() method allows you to specify a maximum length for the generated sentence:"
      ],
      "metadata": {
        "id": "wjChT6F1--zg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(generator_a.make_short_sentence(50))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8VIrlGuqFs_",
        "outputId": "a46171d7-ac8a-4f48-f18c-74fde74ea6c3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "She stared at him in respect of his religion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generator_a.make_short_sentence(40, tries=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFJlXpYh-fqa",
        "outputId": "73b9513f-e773-4b67-839c-17472f98ed21"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rachel, stealing to the men gave way.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generator_a.make_short_sentence(40, test_output=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8ff008o-uJy",
        "outputId": "9d55a660-4481-4243-e1c6-6830ade2ff1d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHAPTER XLIV.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Changing the order**\n",
        "When you create the model, you can specify the order of the model using the state_size parameter. It defaults to 2. Let's make two model with different orders and compare:"
      ],
      "metadata": {
        "id": "_P7QfFTy_RXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gen_a_1 = markovify.Text(text_a, state_size=1)\n",
        "gen_a_4 = markovify.Text(text_a, state_size=4)"
      ],
      "metadata": {
        "id": "YwiRR4Y9-02C"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"order 1\")\n",
        "print(gen_a_1.make_sentence(test_output=False))\n",
        "print()\n",
        "print(\"order 4\")\n",
        "print(gen_a_4.make_sentence(test_output=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgcWVWTn_XdC",
        "outputId": "3f92662e-f8e7-433d-9f11-288ec42e9bad"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "order 1\n",
            "That upon his judgment upon his eyes.\n",
            "\n",
            "order 4\n",
            "Of such stuff are martyrs made; from such elements springs the lofty ideal into which, once in a generation, is breathed the breath of life, the self-sacrificing hero who sheds his blood and dies with a glad light on his face in the battle of weak innocence against the ruthless hand of power.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Changing the level**\n",
        "Markovify, by default, works with words as the individual unit. It doesn't come out-of-the-box with support for character-level models. The following code defines a new kind of Markovify generator that implements character-level models. Execute it before continuing:"
      ],
      "metadata": {
        "id": "U0TIfLFy_iFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentencesByChar(markovify.Text):\n",
        "    def word_split(self, sentence):\n",
        "        return list(sentence)\n",
        "    def word_join(self, words):\n",
        "        return \"\".join(words)"
      ],
      "metadata": {
        "id": "muBcVy40_agK"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Any of the parameters you passed to markovify.Text you can also pass to SentencesByChar. The state_size parameter still controls the order of the model, but now the n-grams are characters, not words."
      ],
      "metadata": {
        "id": "lZB5ZJ1TAAyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "con_model = SentencesByChar(\"condescendences\", state_size=2)"
      ],
      "metadata": {
        "id": "1T_aETCH_tBT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute the cell below to see the output—it'll be a lot like what we implemented by hand earlier!"
      ],
      "metadata": {
        "id": "_xv92EhCAQbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "con_model.make_sentence()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "2Gy97AA2AMAz",
        "outputId": "d92b4607-44c5-475e-ebea-1ff2a2e0a003"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'condendescescendesces'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, you can use a character-level model on any text of your choice. So, for example, the following cell creates a character-level order-7 Markov chain text generator from text A:"
      ],
      "metadata": {
        "id": "ebgKed3HAhFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gen_a_char = SentencesByChar(text_a, state_size=7)"
      ],
      "metadata": {
        "id": "zZ4-RD03AbVD"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And the cell below prints out a random sentence from this generator. (The .replace() is to get rid of any newline characters in the output.)"
      ],
      "metadata": {
        "id": "EZxkak_kAtuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(gen_a_char.make_sentence(test_output=False).replace(\"\\n\", \" \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKwtxGxhAmbC",
        "outputId": "7aab4329-0f79-4811-ff98-f7e2c1f0fea6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are about it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Combining models**\n",
        "Markovify has a handy feature that allows you to combine models, creating a new model that draws on probabilities from both of the source models. You can use this to create hybrid output that mixes the style and content of two (or more!) different source texts. To do this, you need to create the models independently, and then call .combine() to combine them."
      ],
      "metadata": {
        "id": "vi2axUalA1mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator_a = markovify.Text(text_a)\n",
        "generator_b = markovify.Text(text_b)\n",
        "combo = markovify.combine([generator_a, generator_b], [0.5, 0.5])"
      ],
      "metadata": {
        "id": "cHtFpLuiApvy"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bit of code [0.5, 0.5] controls the \"weights\" of the models, i.e., how much to emphasize the probabilities of any model. You can change this to suit your tastes. (E.g., if you want mostly text A with but a soupçon of text B, you would write [0.9, 0.1]. Try it!)\n",
        "\n",
        "Then you can create sentences using the combined model:"
      ],
      "metadata": {
        "id": "YTbJsxuWA8Z5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(combo.make_sentence())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atTI3BL8A718",
        "outputId": "42bd0528-88b9-4e62-9849-59f1a30b296d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It is a necessity for impressing it upon him.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We all know that his religion and taught to look on the subject, but that the arguments with which I feared to meet her again."
      ],
      "metadata": {
        "id": "mmErhOGjBOdv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bringing it all together**\n",
        "I've pre-written some code below to make it easy for you to experiment and produce output from Markovify. Just make adjustments to the values assigned to the variables in the cell below:"
      ],
      "metadata": {
        "id": "NJ5wETkSBTZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# change to \"word\" for a word-level model\n",
        "level = \"char\"\n",
        "# controls the length of the n-gram\n",
        "order = 7\n",
        "# controls the number of lines to output\n",
        "output_n = 14\n",
        "# weights between the models; text A first, text B second.\n",
        "# if you want to completely exclude one model, set its corresponding value to 0\n",
        "weights = [0.5, 0.5]\n",
        "# limit sentence output to this number of characters\n",
        "length_limit = 280"
      ],
      "metadata": {
        "id": "uhPjzH1XBOI2"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(The lines beginning with # are \"comments\"—they don't do anything, they're just there to explain what's happening in the code.)\n",
        "\n",
        "After making your changes above, run the cell below to generate text according to your parameters. Repeat as necessary until you get something you really like!"
      ],
      "metadata": {
        "id": "5SiWbklZBd6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_cls = markovify.Text if level == \"word\" else SentencesByChar\n",
        "gen_a = model_cls(text_a, state_size=order)\n",
        "gen_b = model_cls(text_b, state_size=order)\n",
        "gen_combo = markovify.combine([gen_a, gen_b], weights)\n",
        "for i in range(output_n):\n",
        "    out = gen_combo.make_short_sentence(length_limit, test_output=False)\n",
        "    out = out.replace(\"\\n\", \" \")\n",
        "    print(out)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8W-JVO17BLHQ",
        "outputId": "a15449e1-fed6-4c90-a008-0ded983a5896"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blessed with the name of them were the rear of the poor.\n",
            "\n",
            "His argument, because you if you could do was to make is--and a worthy endeavour to her, should inhabitant when I think, to act in a holiday times, or from burglars, there was angry at the point in Aaron was waiting for.\n",
            "\n",
            "She, on her aunt's shop, found that they gazed up and donations, and heard the wilderness.\n",
            "\n",
            "There is another.\n",
            "\n",
            "Not a harsh thought it was electronically precious to obtain from such unselfishness than flight as to how far the finger is more than a kingdom; but a large a particular state visit has had never did, and the country.\n",
            "\n",
            "Esther Moss, without inconvenience.\n",
            "\n",
            "Simply dressed his should be here the day I received the victory have been something which is the following more than himself in the Bank of England she awoke from the fair vales beneath its roof.\n",
            "\n",
            "I promise to carry out my best workmen and which filled herself.\n",
            "\n",
            "You would reach the son of his wife.\n",
            "\n",
            "With Aaron expectancy of the child.\n",
            "\n",
            "She trembled on that I am about her.\n",
            "\n",
            "To bring joy in Prince's Gate, which he looked at the centre of strength.\n",
            "\n",
            "Setting him out of his pocket, and ready written letter, enclosure that Rachel, but Mr. Gordon's veins ran the sofa before her.\n",
            "\n",
            "He had been so palatable the astonishment between had been since the night, and had obeyed; he had been born?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating with non-prose text\n",
        "Markovify assumes you're feeding it prose, i.e., a text file that can be parsed into sentences by separating on sentence-ending punctuation. But often you're not working with text like this. For example, let's generate some sonnets. First, download this plaintext version of Shakespeare's sonnets and keep it in the same directory as this notebook. We'll define the sonnet-generating task as consisting of (a) training a Markov chain on lines of poetry and then (b) generating a sequence of fourteen lines of poetry. Since the line is the unit now and not the sentence, we need to use Markovify's NewlineText class instead of Text:"
      ],
      "metadata": {
        "id": "cgwfg9sXBvaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sonnets_text = open(\"sonnets.txt\").read()\n",
        "sonnets_model = markovify.NewlineText(sonnets_text, state_size=1)"
      ],
      "metadata": {
        "id": "qew43ZyhBqVD"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then we can generate:"
      ],
      "metadata": {
        "id": "jfpVccqpClhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sonnets_model.make_sentence()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aZ2C1znCB1sr",
        "outputId": "a0c3a95e-4ec1-455a-c812-15c85562ea46"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'To give physic to make,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now make a sonnet, sorta:"
      ],
      "metadata": {
        "id": "wpyYVifHCt8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(14):\n",
        "    print(sonnets_model.make_sentence())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIWzwsNhCoxs",
        "outputId": "b67a294a-8cfb-4772-e0f6-4066eefe561d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "And, thou the travail of life, and fell sick withal, the proudest sail to life, being dead;\n",
            "Serving with eyes and think on all,\n",
            "And threescore year would corrupt by this change my lover's life:\n",
            "When swift foot back?\n",
            "Why should nothing that I hallow'd thy self uprear,\n",
            "And make those.\n",
            "And heavily he so possess'd with outward praise that I have been before,\n",
            "As, thou not at pleasure of seldom coming end you yourself may character,\n",
            "With my soul's thought,\n",
            "And in the blind fool, Love, what wealth she stores, to remove:\n",
            "Which my rose,\n",
            "Save that, to flow,\n",
            "Since every where.\n",
            "Nor think the day?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doing this with a character-level model is a bit more tricky. I've written code in the cell below that defines a new class, LinesByCharacter that works like NewlineText but operates character-by-character instead of word-by-word:"
      ],
      "metadata": {
        "id": "9dxkdMTXC6gL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinesByChar(markovify.NewlineText):\n",
        "    def word_split(self, sentence):\n",
        "        return list(sentence)\n",
        "    def word_join(self, words):\n",
        "        return \"\".join(words)"
      ],
      "metadata": {
        "id": "mxSctHu1C2oa"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can create a character model with the sonnets, line by line:"
      ],
      "metadata": {
        "id": "uNcfYDj8DDEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sonnets_char_model = LinesByChar(sonnets_text, state_size=4)"
      ],
      "metadata": {
        "id": "Prcte9R9C_iS"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And generate new sonnets:"
      ],
      "metadata": {
        "id": "enSmi4UyDJx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(14):\n",
        "    print(sonnets_char_model.make_sentence())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srOJD1rfDG8r",
        "outputId": "03ac86a5-8d24-4cb1-88e9-9a856cc94619"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For the perfumes of stage,\n",
            "For I hast dost his sworn, thy child, those is, by thou should be.\n",
            "Knowing best my nature of all his to gazers remain,\n",
            "Happy your me upon dwell-seeing sights,\n",
            "Her proud as pity of thy delight,\n",
            "This they knight,\n",
            "Since vouches go,\n",
            "Beshrew acquainting Time do cries:\n",
            "Praise is they loss,\n",
            "Was it was of he lamb he trouble-vantage for tented face\n",
            "When let not to thee view;\n",
            "Before I sweet beauty tell;\n",
            "O! less gone\n",
            "Anothere onsecrate:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **New moods**\n",
        "Character-level Markov chains are especially suitable, in my experience, for generating shorter texts, like individual words or names. Let's generate names of new moods using this technique. First, download this JSON file of moods from Corpora Project and save to the same directory as this notebook.\n",
        "\n",
        "Then load the JSON file and grab just the list of words naming moods:"
      ],
      "metadata": {
        "id": "j9sA5D6UDXrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "mood_data = json.loads(open(\"./moods.json\").read())\n",
        "moods = mood_data['moods']"
      ],
      "metadata": {
        "id": "F_r_adhJDNDT"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The easiest way to use this is to make one big string with the moods joined together with newlines:"
      ],
      "metadata": {
        "id": "T-Cyhls1Dy5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "moods_text = \"\\n\".join(moods)"
      ],
      "metadata": {
        "id": "9SOYL5LwDtW6"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then use LinesByChar to make the model:"
      ],
      "metadata": {
        "id": "tIOGE45kD5Fz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "moods_char_model = LinesByChar(moods_text, state_size=3)"
      ],
      "metadata": {
        "id": "qrmRM5vDD4Za"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And voila, new moods:"
      ],
      "metadata": {
        "id": "DLUomE9PEEIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(24):\n",
        "    print(moods_char_model.make_sentence())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mh8p-91cEBaX",
        "outputId": "a10132b0-dabe-4cc8-aa6d-29a2d047eff5"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "impeless\n",
            "introng\n",
            "powerloadequashed\n",
            "overwhelplenished\n",
            "difficated\n",
            "None\n",
            "molenchallow\n",
            "disheepy\n",
            "sombenterrupted\n",
            "revenate\n",
            "suppointronious\n",
            "talienthreased\n",
            "horribald\n",
            "talkatirictory\n",
            "indescued\n",
            "ridicalm\n",
            "warnisconfit\n",
            "neurottenewed\n",
            "melancholistalistled\n",
            "thrillenischievil\n",
            "phonorant\n",
            "rembarratenished\n",
            "peacefulfish\n",
            "guardly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xMBQXUiREGuj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}